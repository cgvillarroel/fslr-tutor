{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "ff756f34",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import cv2\nimport mediapipe as mp\nfrom structs.types import NormalizedLandmark, Frame, Clip, Gesture\n\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_holistic = mp.solutions.holistic"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "d0a5c912",
            "metadata": {
                "lines_to_next_cell": 2,
                "trusted": true
            },
            "outputs": [],
            "source": "# util: draw landmarks\ndraw_landmarks = False\n\n\ndef drawLandmarks(image, results):\n    # Draw landmark annotation on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    mp_drawing.draw_landmarks(\n        image,\n        results.face_landmarks,\n        mp_holistic.FACEMESH_CONTOURS,\n        landmark_drawing_spec=None,\n        connection_drawing_spec=mp_drawing_styles\n        .get_default_face_mesh_contours_style())\n    mp_drawing.draw_landmarks(\n        image,\n        results.pose_landmarks,\n        mp_holistic.POSE_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles\n        .get_default_pose_landmarks_style())\n    mp_drawing.draw_landmarks(\n        image,\n        results.left_hand_landmarks,\n        mp_holistic.HAND_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles\n        .get_default_pose_landmarks_style())\n    mp_drawing.draw_landmarks(\n        image,\n        results.right_hand_landmarks,\n        mp_holistic.HAND_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles\n        .get_default_pose_landmarks_style())\n\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow(\"landmarks\", cv2.flip(image, 1))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "a55eb4e3",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# util: extract landmarks\n# no longer needed, rescaled with ffmpeg\n# downsample_width = 320\n# downsample_height = 240\n\n\ndef extractLandmarks(file_name):\n    cap = cv2.VideoCapture(file_name)\n    results = []\n    with mp_holistic.Holistic(\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5,\n            model_complexity=0) as holistic:\n        while cap.isOpened():\n            success, frame = cap.read()\n            if not success:\n                break\n\n            # cv2.resize(frame, (downsample_width, downsample_height),\n            #            interpolation=cv2.INTER_LINEAR)\n\n            # To improve performance, optionally mark the image as not writeable to pass by reference.\n            frame.flags.writeable = False\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            result = holistic.process(frame)\n            results.append(result)\n\n            if draw_landmarks:\n                drawLandmarks(frame, result)\n\n            if cv2.waitKey(5) == ord(\"q\"):\n                break\n    cap.release()\n    cv2.destroyAllWindows()\n    return results"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "1204a895",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# util: normalize\n\n\ndef normalizeResults(results):\n    normalized_results = Clip()\n    for result in results:\n        min_x = float(\"inf\")\n        min_y = float(\"inf\")\n        max_x = float(\"-inf\")\n        max_y = float(\"-inf\")\n\n        landmarks = result.pose_landmarks.landmark\n\n        for landmark in landmarks:\n            min_x = landmark.x if landmark.x < min_x else min_x\n            min_y = landmark.y if landmark.y < min_y else min_y\n            max_x = landmark.x if landmark.x > max_x else max_x\n            max_y = landmark.y if landmark.y > max_y else max_y\n\n        normalized_landmarks = Frame()\n\n        if result.pose_landmarks is not None:\n            for landmark in result.pose_landmarks.landmark:\n                normalized_x = (landmark.x - min_x) / (max_x - min_x)\n                normalized_y = (landmark.y - min_y) / (max_y - min_y)\n                normalized_landmarks.pose_landmarks.append(NormalizedLandmark(normalized_x, normalized_y, landmark.visibility))\n\n        if result.face_landmarks is not None:\n            for landmark in result.face_landmarks.landmark:\n                normalized_x = (landmark.x - min_x) / (max_x - min_x)\n                normalized_y = (landmark.y - min_y) / (max_y - min_y)\n                normalized_landmarks.face_landmarks.append(NormalizedLandmark(normalized_x, normalized_y, landmark.visibility))\n\n        if result.left_hand_landmarks is not None:\n            for landmark in result.left_hand_landmarks.landmark:\n                normalized_x = (landmark.x - min_x) / (max_x - min_x)\n                normalized_y = (landmark.y - min_y) / (max_y - min_y)\n                normalized_landmarks.left_hand_landmarks.append(NormalizedLandmark(normalized_x, normalized_y, landmark.visibility))\n\n        if result.right_hand_landmarks is not None:\n            for landmark in result.right_hand_landmarks.landmark:\n                normalized_x = (landmark.x - min_x) / (max_x - min_x)\n                normalized_y = (landmark.y - min_y) / (max_y - min_y)\n                normalized_landmarks.right_hand_landmarks.append(NormalizedLandmark(normalized_x, normalized_y, landmark.visibility))\n\n        normalized_results.frames.append(normalized_landmarks)\n\n    return normalized_results"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "e7e726f0",
            "metadata": {
                "lines_to_next_cell": 1,
                "trusted": true
            },
            "outputs": [],
            "source": "# actual preprocessing\n# save to a binary in chunks to avoid repeating\nimport os\nimport pickle\n\n\ndataset_path = \"dataset\"\nclips_path = f\"{dataset_path}/scaled_clips\"\nchunks_path = f\"{dataset_path}/chunks\"\n\ndef preprocessRange(start, stop):\n    for gesture_idx in range(start, stop):\n        gesture = Gesture()\n        clip_count = len(next(os.walk(f\"{clips_path}/{gesture_idx}\"))[2])\n        for clip_idx in range(clip_count):\n            result = extractLandmarks(f\"{clips_path}/{gesture_idx}/{clip_idx}.MOV\")\n            normalized_result = normalizeResults(result)\n            gesture.clips.append(normalized_result)\n\n        # redundancy in case of failure\n        with open(f\"{chunks_path}/{gesture_idx}.pkl\", \"wb\") as chunk_writer:\n            pickle.dump(gesture, chunk_writer)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0d6dcb0",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Processing gesture 0, clip 0    \rProcessing gesture 5, clip 0    \rProcessing gesture 10, clip 0    \rProcessing gesture 15, clip 0    \rProcessing gesture 20, clip 0    \rProcessing gesture 25, clip 0    \rProcessing gesture 30, clip 0    \rProcessing gesture 35, clip 0    \rProcessing gesture 40, clip 0    \rProcessing gesture 45, clip 0    \rProcessing gesture 50, clip 0    \rProcessing gesture 55, clip 0    \rProcessing gesture 60, clip 0    \rProcessing gesture 65, clip 0    \rProcessing gesture 70, clip 0    \rProcessing gesture 75, clip 0    \rProcessing gesture 80, clip 0    \rProcessing gesture 85, clip 0    \rProcessing gesture 90, clip 0    \rProcessing gesture 95, clip 0    \rProcessing gesture 100, clip 0    \r"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "D:\\repos\\thesis\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Processing gesture 100, clip 4    rocessing gesture 45, clip 2    Processing gesture 95, clip 2    \r"
                }
            ],
            "source": "import threading\n\nthreads = []\nfor idx in range(10):\n    threads.append(threading.Thread(target=preprocessRange, args=(idx * 10, idx * 10 + 10)))\n\nthreads.append(threading.Thread(target=preprocessRange,args=(100,105)))\n\nfor thread in threads:\n    thread.start()\n\nfor thread in threads:\n    thread.join()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "207b01f3",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# sample reading\nwith open(f\"{dataset_path}/preprocessed.pkl\", \"rb\") as reader:\n    loaded_gesture: Gesture = pickle.load(reader)\n    for landmark in loaded_gesture.clips[0].frames[0].pose_landmarks:\n        print(f\"norm x: {landmark.x:.5f}, norm y: {landmark.y:.5}, visibility: {landmark.visibility:.5f}\")"
        }
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "main_language": "python",
            "notebook_metadata_filter": "-all"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}